{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. It was mentioned in the chapter that a cubic regression spline with one knot at `ξ` can be obtained using a basis of the form `x, x2, x3, (x−ξ)^3+`, where `(x−ξ)^3+ = (x−ξ)^3 if x > ξ and equals 0 otherwise`. We will now show that a function of the form:\n",
    "\n",
    "`f(x)=β0 +β1x+β2x2 +β3x3 +β4(x−ξ)3+`\n",
    "\n",
    "is indeed a cubic regression spline, regardless of the values of `β0, β1, β2,β3,β4.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Find a cubic polynomial `f1(x)= a1 + b1x + c1x2 + d1x3` such that `f(x) = f1(x)` for all `x ≤ ξ`. Express `a1,b1,c1,d1` in terms of `β0, β1, β2, β3, β4.`\n",
    "\n",
    "\n",
    "(b) Find a cubic polynomial `f2(x) = a2 + b2x + c2x2 + d2x3` such that `f(x) = f2(x)` for all `x > ξ`. Express `a2, b2 , c2, d2` in terms of `β0, β1, β2, β3, β4`. We have now established that `f(x)` is a piecewise polynomial.\n",
    "\n",
    "(c) Show that `f1(ξ) = f2(ξ)`. That is, `f(x)` is continuous at `ξ`.\n",
    "\n",
    "(d) Show that `f1′(ξ) = f2′(ξ)`. That is, `f′(x)` is continuous at `ξ`.\n",
    "\n",
    "(e) Show that `f′′(ξ) = f′′(ξ)`. That is, `f′′(x) is continuous at ξ.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Q1](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch7_statistical_learning/q1.jpeg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Suppose that a curve gˆ is computed to smoothly fit a set of n points\n",
    "using the following formula: `SEE ISL p298`\n",
    "\n",
    "where `g(m)` represents the `mth` derivative of `g` (and `g(0) = g`). Provide example sketches of `gˆ` in each of the following scenarios.\n",
    "\n",
    "(a) `λ=∞,m=0`. \n",
    "\n",
    "(b) `λ=∞,m=1`.\n",
    "\n",
    "(c) `λ=∞,m=2`.\n",
    "\n",
    "(d) `λ=∞,m=3`.\n",
    "\n",
    "(e) `λ=0,m=3`.\n",
    "\n",
    "3. Suppose we fit a curve with basis functions:\n",
    "\n",
    "`b1(X) = X`\n",
    "\n",
    "`b2(X) = (X − 1)2I(X ≥ 1)`. \n",
    "\n",
    "(Note that `I(X ≥ 1)` equals 1 for `X ≥ 1` and 0 otherwise.) We fit the linear regression model`Y = β0 +β1b1(X)+β2b2(X)+ε` and obtain coefficient estimates `βˆ0 = 1, βˆ1 = 1, βˆ2 = −2`. Sketch the estimated curve between `X = −2` and `X = 2`. Note the intercepts, slopes, and other relevant information.\n",
    "\n",
    "4. Suppose we fit a curve with basis functions:\n",
    "\n",
    "`b1(X) = I(0 ≤ X ≤ 2) − (X −1)I(1 ≤ X ≤ 2)`\n",
    "\n",
    "`b2(X) = (X −3)I(3 ≤ X ≤ 4)+I(4 < X ≤ 5)`.\n",
    "\n",
    "We fit the linear regression model: `Y = β0 +β1b1(X)+β2b2(X)+ε` and obtain coefficient estimates `βˆ0 = 1,βˆ1 = 1,βˆ2 = 3`. Sketch the estimated curve between `X = −2 and X = 2`. Note the intercepts, slopes, and other relevant information.\n",
    "\n",
    "5. Consider two curves, gˆ1 and gˆ2, defined by: `SEE ISL p299` where `g(m)` represents the `mth` derivative of `g`.\n",
    "\n",
    "(a) As `λ → ∞`, will `gˆ1` or `gˆ2` have the smaller training RSS? \n",
    "\n",
    "(b) As `λ → ∞`, will `gˆ1` or `gˆ2` have the smaller test RSS?\n",
    "\n",
    "(c) For `λ = 0`, will `gˆ1` or `gˆ2` have the smaller training and test RSS?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Q2345](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch7_statistical_learning/q2345.jpeg?raw=true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
